# Label Investigation - Final Report

**Date**: 2026-01-13  
**Investigation**: Why internal (51.8% acc) vs external (69.6% acc)?

---

## ğŸ¯ ROOT CAUSE IDENTIFIED: Winner Label Bias

### Summary

The external dataloader has a **systematic bias toward winner=1** that doesn't exist in the internal dataloader.

---

## ğŸ“Š Key Statistics

### Dataset Overlap
- **5,384 overlapping pairs** (42.9% of pairs exist in both loaders)
- Internal total: 10,874 pairs (train+val)
- External total: 12,558 pairs (train+val)

### Label Consistency
- **51.2% labels match** (2,755 pairs)
- **48.8% labels inverted** (2,629 pairs)  
- **0% other mismatches**

### Winner Distribution **IN OVERLAPPING PAIRS**

| Loader | winner=0 | winner=1 | Balance |
|--------|----------|----------|---------|
| Internal | 2,574 (47.8%) | 2,810 (52.2%) | âœ… Balanced |
| External | 1,773 (32.9%) | **3,611 (67.1%)** | âŒ **Heavily biased!** |

**Critical finding**: External has **801 MORE winner=1 labels** on the same 5,384 pairs!

---

## ğŸ” Pattern Analysis

### Cross-Tabulation

```
                 External Winner
                 0      1      Total
Internal   0    859   1715    2574
Winner     1    914   1896    2810
           Total 1773  3611   5384
```

### Interpretation

**When internal winner = 0**:
- External agrees (winner=0): 859 times (33.4%)
- External disagrees (winner=1): **1,715 times (66.6%)** â† Majority inverted!

**When internal winner = 1**:
- External disagrees (winner=0): 914 times (32.5%)
- External agrees (winner=1): **1,896 times (67.5%)** â† Majority match!

### What This Means

This is **NOT** a simple global label inversion (winner 0â†”1).

Instead, there's a **systematic bias** in the external loader:
- External prefers winner=1 regardless of what internal says
- When internal=1, external agrees 67.5% of the time
- When internal=0, external **inverts** to 1 in 66.6% of cases

---

## ğŸ’¡ Possible Explanations

### 1. Image Pair Order Swapping (Most Likely)

**Hypothesis**: External sometimes swaps image1/image2 order for the same pairs.

Example:
- Internal has: (imageA, imageB, winner=0) meaning imageA wins
- External has: (imageB, imageA, winner=1) meaning imageB wins (same winner, swapped order)

This would explain:
- Why ~50% labels differ (swapped vs not swapped)
- Why external favors winner=1 (swapping flips the label)

**To verify**: Check if external's pathA/pathB creation in `make_shuffle_path` sometimes reverses the order compared to internal's image1/image2.

### 2. Winner Label Semantics Differ

**Hypothesis**: winner=0 and winner=1 mean different things in each loader.

Possibilities:
- Internal: winner=0 means "image1 wins"
- External: winner=0 means "image2 wins" (or vice versa)

**To verify**: Manually inspect a few pairs and check which image actually won according to human labels.

### 3. Bug in make_shuffle_path

**Hypothesis**: The external `make_shuffle_path.make_shuffle_path()` has a bug that systematically biases labels.

**To verify**: Inspect the `make_shuffle_path` source code to understand how it assigns winner labels.

---

## ğŸš¨ Impact on Model Performance

### Why External Performs Better (69.6% vs 51.8%)

The **label bias creates an easier learning task**:

1. **Simpler decision boundary**: With 67% of pairs labeled as winner=1, the model can achieve ~67% accuracy by simply predicting winner=1 most of the time

2. **Less balanced training**: The external loader's bias toward winner=1 means:
   - Easier to fit (biased data is often easier to memorize)
   - But potentially less generalizable (overfitting to the bias)

3. **Different effective dataset**: The label flipping creates essentially different training data, so we're not comparing apples to apples

### Why Internal Performs Poorly (51.8% â‰ˆ Random)

The internal loader might be:
1. **Correct but harder**: Balanced labels (47.8% / 52.2%) are harder to learn
2. **Or has its own issues**: Possibly incorrect label interpretation

---

## ğŸ”¬ Next Steps

### Immediate Actions

1. **Investigate `make_shuffle_path.make_shuffle_path()`**:
   ```python
   # Check how it creates pairs and assigns winner labels
   # Look for: pathA, pathB, result arrays
   # Verify: Does it ever swap image order?
   ```

2. **Check Internal PhotoTriageData**:
   ```python
   # Verify how it interprets winner=0 vs winner=1
   # Check: Does winner=0 always mean image1 wins?
   ```

3. **Manual Label Verification** (Ground Truth):
   - Pick 10 pairs from overlapping set
   - Check original CSV: `photo_triage_pairs_embedding_labels.csv`
   - Verify which image ACTUALLY won according to human labels
   - Compare with what internal and external loaders say

### Test Hypothesis

**If image order swapping is the issue**:
- Modify internal loader to match external's pair order
- OR modify external to match internal's pair order
- Retrain and check if accuracy converges

**If winner semantics differ**:
- Fix the interpretation in one loader to match the other
- Retrain and verify

---

## ğŸ“ Files Generated

### Data
- `outputs/label_verification/all_overlapping_pairs_comprehensive.csv` - All 5,384 overlapping pairs with winner comparison
- `outputs/label_verification/summary_comprehensive.json` - Statistics

### Scripts
- `scripts/verify_labels_fast.py` - Fast label verification (used)
- `scripts/verify_transform_application.py` - Transform verification (transforms are identical)

---

## âœ… Confirmed Facts

1. âœ… **Transforms are identical** (0.000000 difference)
2. âœ… **42.9% of pairs overlap** between loaders (5,384 / 12,558)
3. âœ… **External is heavily biased** toward winner=1 (67.1% vs 47.8%)
4. âœ… **NOT a simple label inversion** (pattern is more complex)

---

## â“ Open Questions

1. â“ Does `make_shuffle_path` swap image order for some pairs?
2. â“ What does winner=0 mean in each loader's interpretation?
3. â“ Which loader has the **correct** labels according to original human annotations?
4. â“ Why do loaders only share 42.9% of pairs? (Different pair selection logic)

---

## ğŸ¯ Recommendation

**STOP HERE** and investigate `make_shuffle_path` source code before proceeding.

The performance difference (69.6% vs 51.8%) is **NOT** due to:
- âŒ Transform differences (verified identical)
- âŒ Model architecture
- âŒ Hyperparameters

It **IS** due to:
- âœ… Different training data (only 42.9% overlap)
- âœ… **Systematic label bias in external loader** (67% winner=1)

**Once we understand the label bias, we can fix it and get consistent results across both loaders.**
