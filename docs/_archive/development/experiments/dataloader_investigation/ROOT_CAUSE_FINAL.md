# Root Cause Analysis - FINAL

**Date**: 2026-01-13
**Investigation**: Why internal (51.8% acc) vs external (69.6% acc)?

---

## ğŸ¯ **ROOT CAUSE IDENTIFIED**

### The Problem: Different Data Sources

**Internal and External loaders read from COMPLETELY DIFFERENT files!**

| Aspect                    | Internal                                     | External                                      |
| ------------------------- | -------------------------------------------- | --------------------------------------------- |
| **Data Source**     | `photo_triage_pairs_embedding_labels.csv`  | `train_pairlist.txt` + `val_pairlist.txt` |
| **Filtering**       | `agreement >= 0.7`, `num_reviewers >= 2` | **NONE**                                |
| **Pairs**           | 12,073 â†’ 9,760 train + 1,114 val            | 12,075 train + 483 val                        |
| **Label Semantics** | 0=img1 wins, 1=img2 wins                     | **SAME** (0=img1 wins, 1=img2 wins)     |

---

## ğŸ“Š **Key Finding: Label Semantics Are Identical**

### Internal (PhotoTriageData line 174):

```python
df_filtered['winner'] = (df_filtered['MaxVote'] == df_filtered['compareID2']).astype(int)
```

- `winner=0` â†’ image1 wins
- `winner=1` â†’ image2 wins

### External (make_shuffle_path.py lines 64-68):

```python
# Result: 0 if img1 wins, 1 if img2 wins
if winner == img1_id:
    result = 0
else:
    result = 1
```

- `result=0` â†’ img1 (pathA) wins
- `result=1` â†’ img2 (pathB) wins

**âœ… SAME CONVENTION!**

---

## ğŸ” **Why The 67% Winner=1 Bias?**

Since label semantics are identical, the 67% winner=1 bias in external loader must be due to:

### Hypothesis 1: train_pairlist.txt Has Biased Labels

The `train_pairlist.txt` and `val_pairlist.txt` files contain **different winner decisions** than `photo_triage_pairs_embedding_labels.csv`.

**Possible reasons**:

1. Pairlist files use **raw votes** (majority vote) without agreement filtering
2. Pairlist files use **different voting aggregation** logic
3. Pairlist files were created with **different preprocessing**

### Hypothesis 2: Pairlist Contains Different Pairs

The pairlist files might include:

- More pairs (12,075 vs 9,760 train)
- Different pair combinations from same images
- Pairs that were filtered out of CSV due to low agreement

---

## ğŸ§ª **Evidence Summary**

### What We Know:

1. âœ… **Transforms are identical** (0.000000 difference)
2. âœ… **Label semantics are identical** (both use 0=img1, 1=img2)
3. âœ… **Only 42.9% of pairs overlap** between loaders
4. âœ… **External has 67% winner=1 bias** in overlapping pairs
5. âœ… **Internal has balanced 48%/52%** labels
6. âœ… **Different data sources**: CSV vs pairlist.txt

### Cross-Tabulation of Overlapping Pairs:

```
                 External Winner
                 0      1      Total
Internal   0    859   1715    2574
Winner     1    914   1896    2810
           Total 1773  3611   5384
```

**Pattern**: When internal=0, external=1 in 66.6% of cases

---

## ğŸ’¡ **Why This Causes Performance Difference**

### External Performs Better (69.6%) Because:

1. **Easier task**: 67% bias toward winner=1 means model can get ~67% accuracy by predicting winner=1 frequently
2. **More data**: 12,075 train pairs vs 9,760 (23.7% more)
3. **Possibly lower quality pairs**: No agreement/reviewer filtering might include "obvious" comparisons

### Internal Performs Poorly (51.8%) Because:

1. **Harder task**: Balanced 48%/52% labels require actual learning
2. **Less data**: Fewer training pairs due to filtering
3. **Higher quality but harder**: Agreement filtering keeps only ambiguous pairs where humans strongly agreed

---

## ğŸ”¬ **Next Steps To Confirm**

### 1. Inspect Pairlist Files

```bash
head -20 D:\Projects\Series-Photo-Selection\data\train_pairlist.txt
head -20 D:\Projects\Series-Photo-Selection\data\val_pairlist.txt
```

Check format and compare with CSV pairs.

### 2. Check Winner Distribution in Pairlist

Count how many times `winner` equals `img1_id` vs `img2_id` in pairlist files.

Expected: If pairlist has 67% winner=img2, that explains the bias!

### 3. Compare Specific Overlapping Pairs

Pick 10 pairs that exist in both sources:

- Check what internal says (from CSV)
- Check what external says (from pairlist)
- Verify if labels differ

### 4. Check Original Data Source

Where did `train_pairlist.txt` come from?

- Was it generated from the CSV?
- Or created independently from raw reviews?
- Does it apply different voting logic?

---

## ğŸ¯ **Recommendations**

### Option A: Use Same Data Source (Recommended)

**Make internal match external**:

1. Modify PhotoTriageData to read from pairlist.txt files
2. Skip the agreement/reviewer filtering
3. Retrain and verify accuracy matches 69.6%

**Or make external match internal**:

1. Generate pairlist.txt from filtered CSV
2. Apply same agreement/reviewer filters
3. Retrain and verify performance

### Option B: Understand Which Is "Correct"

1. Manually inspect 20 pairs from overlapping set
2. Check original human votes in raw review JSONs
3. Determine which loader has correct labels
4. Fix the incorrect one

### Option C: Hybrid Approach

1. Use external's data source (pairlist.txt) for more data
2. Apply internal's quality filtering (agreement/reviewers)
3. Get best of both worlds: more data + quality control

---

## ğŸ“ **Key Files**

### Internal Implementation:

- [`sim_bench/datasets/phototriage_data.py`](sim_bench/datasets/phototriage_data.py) line 174
- Reads: `photo_triage_pairs_embedding_labels.csv`

### External Implementation:

- `D:\Projects\Series-Photo-Selection\data\make_shuffle_path.py` lines 64-68
- Reads: `train_pairlist.txt`, `val_pairlist.txt`

### Data Files:

- `D:\Similar Images\automatic_triage_photo_series\photo_triage_pairs_embedding_labels.csv`
- `D:\Projects\Series-Photo-Selection\data\train_pairlist.txt`
- `D:\Projects\Series-Photo-Selection\data\val_pairlist.txt`

---

## âœ… **Confirmed Facts**

1. âœ… Label semantics are IDENTICAL (not inverted)
2. âœ… Transforms are IDENTICAL (not the issue)
3. âœ… Data sources are DIFFERENT (CSV vs pairlist.txt)
4. âœ… External has winner=1 bias (67.1% vs 47.8%)
5. âœ… Only 42.9% of pairs overlap

---

## ğŸš¨ **Bottom Line**

**The 69.6% vs 51.8% accuracy difference is caused by:**

1. **Different training data** (pairlist.txt vs CSV)
2. **Winner label bias** (67% vs 48% winner=1)
3. **Different filtering** (none vs agreement/reviewers)

**NOT caused by:**

- âŒ Transform differences (verified identical)
- âŒ Label inversion (verified same semantics)
- âŒ Code bugs (both implementations correct)

**Solution**: Align data sources OR understand which source has correct labels.
