# Quality Assessment Benchmark Framework

Flexible, extensible framework for benchmarking image quality assessment methods on PhotoTriage and other datasets.

## Quick Start

```bash
# Run quick test (fast, sampled data)
python run_quality_benchmark.py configs/quality_benchmark.quick.yaml

# Run full PhotoTriage benchmark
python run_quality_benchmark.py configs/quality_benchmark.phototriage.yaml

# Generate visualizations
python visualize_quality_benchmark.py outputs/quality_benchmarks/benchmark_2025-11-12_*
```

## Features

✅ **Flexible Configuration** - YAML-based setup for datasets and methods  
✅ **Multiple Methods** - Rule-based, CNN (NIMA), Transformer (MUSIQ)  
✅ **Multiple Datasets** - PhotoTriage, Budapest, or custom datasets  
✅ **Comprehensive Metrics** - Accuracy (Top-1, Top-2, MRR), Speed, Efficiency  
✅ **Automatic Visualizations** - Charts and markdown reports  
✅ **Extensible** - Easy to add new methods or datasets  

## Benchmark Configuration

### Example: Quick Test

```yaml
# configs/quality_benchmark.quick.yaml
datasets:
  - name: phototriage
    config: configs/dataset.phototriage.yaml
    sampling:
      strategy: random
      num_series: 20
      seed: 42

methods:
  - name: composite_rule_based
    type: rule_based
    config:
      weights:
        sharpness: 0.3
        exposure: 0.2
        colorfulness: 0.2
        contrast: 0.15
        noise: 0.15
```

### Example: Full Benchmark

```yaml
# configs/quality_benchmark.phototriage.yaml
datasets:
  - name: phototriage
    config: configs/dataset.phototriage.yaml

methods:
  - name: composite_rule_based
    type: rule_based
    config: {...}
  
  - name: nima_mobilenet
    type: nima
    config:
      backbone: mobilenet
      device: cuda
  
  - name: musiq_aesthetic
    type: musiq
    config:
      variant: ava
      device: cuda
```

## Output Structure

```
outputs/quality_benchmarks/benchmark_2025-11-12_12-34-56/
├── config.yaml                # Config used
├── summary.json              # Complete results
├── methods_summary.csv       # Method comparison table
├── detailed_results.csv      # Per-dataset, per-method
├── [dataset]_results.json    # Full results per dataset
└── [dataset]_[method]_series.json  # Per-series details
```

## Key Metrics

| Metric | Description | Range |
|--------|-------------|-------|
| **Top-1 Accuracy** | % series where best image ranked #1 | 0-1 |
| **Top-2 Accuracy** | % series where best image in top 2 | 0-1 |
| **MRR** | Mean Reciprocal Rank | 0-1 |
| **Time (ms)** | Average processing time per image | - |
| **Efficiency** | Accuracy / Time | - |

## Visualizations

Generated by `visualize_quality_benchmark.py`:

1. **Accuracy Comparison** - Bar charts for Top-1 and Top-2
2. **Speed Comparison** - Processing time comparison
3. **Accuracy vs Speed** - Tradeoff scatter plot
4. **Per-Dataset Comparison** - Individual dataset charts
5. **REPORT.md** - Markdown report with tables and rankings

## Usage Examples

### Development Testing

```bash
# Fast iteration with small sample
python run_quality_benchmark.py configs/quality_benchmark.quick.yaml
```

### Full Evaluation

```bash
# Complete PhotoTriage evaluation
python run_quality_benchmark.py configs/quality_benchmark.phototriage.yaml

# Generate report
python visualize_quality_benchmark.py outputs/quality_benchmarks/benchmark_*
```

### Multi-Dataset Comparison

```bash
# Test across multiple datasets
python run_quality_benchmark.py configs/quality_benchmark.multi_dataset.yaml
```

### Custom Output Location

```bash
python run_quality_benchmark.py config.yaml --output results/my_benchmark/
```

## Adding New Methods

1. Implement method class inheriting from `BaseQualityMethod`:

```python
# sim_bench/quality_assessment/my_method.py
from sim_bench.quality_assessment.base import BaseQualityMethod

class MyMethod(BaseQualityMethod):
    def assess_image(self, image_path):
        # Your implementation
        return quality_score
```

2. Register in `sim_bench/quality_assessment/__init__.py`:

```python
from sim_bench.quality_assessment.my_method import MyMethod

def load_quality_method(method_type, config):
    method_map = {
        'my_method': MyMethod,
        # ... existing methods
    }
    # ...
```

3. Use in config:

```yaml
methods:
  - name: my_custom_method
    type: my_method
    config:
      param1: value1
```

## Adding New Datasets

Your dataset must implement the `BaseDataset` interface:

```python
class MyDataset(BaseDataset):
    def get_images(self):
        """Return list of image paths"""
        return self.image_paths
    
    def get_evaluation_data(self):
        """Return dict with 'series' key containing evaluation data"""
        return {
            'series': [
                {
                    'series_id': 'series_001',
                    'images': ['img1.jpg', 'img2.jpg', 'img3.jpg'],
                    'best_image': 'img2.jpg'  # Ground truth
                },
                # ... more series
            ]
        }
```

Then add to configs:

```yaml
datasets:
  - name: my_dataset
    config: configs/dataset.my_dataset.yaml
```

## Performance Expectations

**PhotoTriage (~2000 series)**:

| Method Type | Time/Image | Full Dataset | Device |
|-------------|------------|--------------|--------|
| Rule-Based | 2-5ms | ~5-10 min | CPU |
| CNN (NIMA) | 20-50ms | ~40-100 min | GPU |
| Transformer | 50-200ms | ~100-400 min | GPU |

**Recommendations**:
- Development: Sample 50-100 series
- Final evaluation: Full dataset
- Use GPU for deep learning methods

## Troubleshooting

**"CUDA out of memory"**
```yaml
config:
  device: cpu  # Switch to CPU
```

**"Benchmark too slow"**
```yaml
sampling:
  num_series: 20  # Use smaller sample
```

**"Dataset not found"**
```bash
# Verify dataset config exists
ls configs/dataset.*.yaml
```

## Documentation

- **[Complete Guide](docs/QUALITY_BENCHMARK_GUIDE.md)** - Detailed documentation
- **[Quick Start](docs/QUALITY_ASSESSMENT_QUICKSTART.md)** - Getting started
- **[Image Selection Survey](docs/IMAGE_SELECTION_SURVEY.md)** - Literature review

## Architecture

```
sim_bench/quality_assessment/
├── __init__.py           # Method loading
├── base.py              # Base class
├── rule_based.py        # Rule-based methods
├── cnn_methods.py       # CNN methods (NIMA)
├── transformer_methods.py  # Transformer methods (MUSIQ)
├── evaluator.py         # Evaluation framework
├── benchmark.py         # Benchmark runner
└── visualization.py     # Charts and reports

configs/
├── quality_benchmark.quick.yaml        # Quick test
├── quality_benchmark.phototriage.yaml  # Full PhotoTriage
└── quality_benchmark.multi_dataset.yaml  # Multi-dataset

Scripts:
├── run_quality_benchmark.py      # Run benchmarks
└── visualize_quality_benchmark.py  # Generate visualizations
```

## Example Output

### Console Output

```
================================================================================
Quality Assessment Benchmark
================================================================================
Output directory: outputs/quality_benchmarks/benchmark_2025-11-12_12-34-56
Datasets: 1
Methods: 3
================================================================================

Dataset: phototriage
================================================================================

Method: composite_rule_based
--------------------------------------------------------------------------------
  Top-1 Accuracy: 0.7234 (72.34%)
  Avg Runtime: 2.34ms per image

Method: nima_mobilenet
--------------------------------------------------------------------------------
  Top-1 Accuracy: 0.7891 (78.91%)
  Avg Runtime: 45.67ms per image

...

================================================================================
BENCHMARK SUMMARY
================================================================================

Method Rankings by Accuracy:
--------------------------------------------------------------------------------
  1. nima_mobilenet                 0.7891 (78.91%)
  2. composite_rule_based           0.7234 (72.34%)
  3. sharpness_only                 0.6789 (67.89%)

Method Rankings by Speed:
--------------------------------------------------------------------------------
  1. sharpness_only                 1.23 ms/image
  2. composite_rule_based           2.34 ms/image
  3. nima_mobilenet                 45.67 ms/image

================================================================================
Results saved to: outputs/quality_benchmarks/benchmark_2025-11-12_12-34-56
================================================================================
```

### CSV Output (methods_summary.csv)

```csv
method,avg_top1_accuracy,avg_top2_accuracy,avg_mrr,avg_time_ms,datasets_tested
nima_mobilenet,0.7891,0.9234,0.8345,45.67,1
composite_rule_based,0.7234,0.8912,0.7823,2.34,1
sharpness_only,0.6789,0.8456,0.7345,1.23,1
```

## Integration with Existing Workflows

The benchmark framework integrates seamlessly with existing sim-bench infrastructure:

- Uses same dataset abstraction (`BaseDataset`)
- Compatible with existing configs
- Similar result management structure
- Can run alongside retrieval benchmarks

## License

Part of sim-bench project.


