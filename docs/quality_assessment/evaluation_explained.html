<!DOCTYPE html>
<html>
<head>
    <title>Understanding Quality Assessment Evaluation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
            border-bottom: 3px solid #333;
            padding-bottom: 10px;
        }
        h2 {
            color: #555;
            margin-top: 30px;
            border-bottom: 2px solid #ccc;
            padding-bottom: 5px;
        }
        h3 {
            color: #777;
            margin-top: 20px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        pre {
            background-color: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #4CAF50;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 2px 4px;
        }
    </style>
</head>
<body>
    <h1>Understanding Quality Assessment Evaluation</h1>
    
    <p>A comprehensive guide to how PhotoTriage evaluation works, with concrete examples.</p>

    <h2>The Basic Setup</h2>

    <h3>What is a Photo Series?</h3>
    <p>When you take multiple photos in quick succession (burst mode), you get a series of similar images. PhotoTriage contains 4,986 such series.</p>

    <p><strong>Example Series:</strong></p>
    <pre>Series 000002: 8 photos of the same moment
- 000002-01.JPG
- 000002-02.JPG
- 000002-03.JPG
- 000002-04.JPG
- 000002-05.JPG
- 000002-06.JPG
- 000002-07.JPG
- 000002-08.JPG</pre>

    <h3>The Ground Truth: How Humans Determine the "Best" Photo</h3>
    <p>For EACH series, humans have labeled which photo is the BEST, but this wasn't done by directly picking one photo. Instead, it was created through <strong>pairwise comparisons</strong>:</p>
    
    <h4>Step 1: Pairwise Comparisons</h4>
    <p>Humans were shown pairs of images and asked: "Which photo is better?"</p>
    <p>For example, with Series 000002 (8 images), humans made many comparisons like:</p>
    <ul>
        <li>Image 2-3.JPG vs 2-4.JPG → Human chose 2-4.JPG</li>
        <li>Image 2-1.JPG vs 2-3.JPG → Human chose 2-3.JPG</li>
        <li>Image 2-5.JPG vs 2-6.JPG → Human chose 2-5.JPG</li>
        <li>... and many more comparisons</li>
    </ul>
    
    <h4>Step 2: Aggregate to Ranking</h4>
    <p>From all these pairwise comparisons, a ranking algorithm (like Bradley-Terry model or Elo rating) determines:</p>
    <ul>
        <li>Which image "wins" most comparisons</li>
        <li>The overall ranking of all images</li>
        <li>The "best" image = the one that wins the most comparisons</li>
    </ul>
    
    <h4>Example: Series 000002</h4>
    <p>Looking at the actual comparison data for Series 000002 (8 images), here's how it works:</p>
    
    <p><strong>Sample comparisons from the data:</strong></p>
    <table>
        <tr>
            <th>Image A</th>
            <th>Image B</th>
            <th>Human Choice</th>
            <th>Reason</th>
        </tr>
        <tr>
            <td>2-4.JPG</td>
            <td>2-3.JPG</td>
            <td>2-4.JPG (RIGHT)</td>
            <td>"you can not see the water" (in 2-3.JPG)</td>
        </tr>
        <tr>
            <td>2-3.JPG</td>
            <td>2-7.JPG</td>
            <td>2-7.JPG (RIGHT)</td>
            <td>"The color seems a little fuzzy" (in 2-3.JPG)</td>
        </tr>
        <tr>
            <td>2-5.JPG</td>
            <td>2-3.JPG</td>
            <td>2-5.JPG (LEFT)</td>
            <td>"the amount of sky, land, and water in the picture are not as balanced" (in 2-3.JPG)</td>
        </tr>
        <tr>
            <td>2-2.JPG</td>
            <td>2-3.JPG</td>
            <td>2-3.JPG (RIGHT)</td>
            <td>"it shows less drama and detail than the other" (in 2-2.JPG)</td>
        </tr>
    </table>
    
    <h4>Step 3: Count Wins</h4>
    <p>After collecting many comparisons (often 100+ per series), count how many times each image "wins":</p>
    <table>
        <tr>
            <th>Image</th>
            <th>Wins</th>
            <th>Losses</th>
            <th>Win Rate</th>
        </tr>
        <tr>
            <td>2-3.JPG</td>
            <td>45</td>
            <td>32</td>
            <td>58.4%</td>
        </tr>
        <tr>
            <td>2-4.JPG</td>
            <td>38</td>
            <td>28</td>
            <td>57.6%</td>
        </tr>
        <tr>
            <td>2-5.JPG</td>
            <td>42</td>
            <td>35</td>
            <td>54.5%</td>
        </tr>
        <tr>
            <td>2-1.JPG</td>
            <td>12</td>
            <td>48</td>
            <td>20.0%</td>
        </tr>
        <tr>
            <td>... (other images)</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
        </tr>
    </table>
    
    <h4>Step 4: Determine Best Image</h4>
    <p>The image with the highest win rate (or highest ranking score) becomes the "best" image for that series.</p>
    <p>In this example, <strong>2-3.JPG would be the best</strong> because it wins 58.4% of its comparisons.</p>
    
    <h4>Why This Method?</h4>
    <ul>
        <li><strong>More reliable:</strong> Comparing two images is easier than ranking 8 at once</li>
        <li><strong>Multiple opinions:</strong> Many humans can contribute comparisons</li>
        <li><strong>Handles ties:</strong> Ranking algorithms can handle inconsistent preferences</li>
        <li><strong>Scalable:</strong> Works for any number of images (2-8)</li>
    </ul>
    
    <h4>What This Means for Evaluation</h4>
    <p>When we evaluate your algorithm:</p>
    <ul>
        <li>We don't have access to the pairwise comparison data</li>
        <li>We only know the final "best" image (the one that won the most comparisons)</li>
        <li>Your algorithm must rank ALL images and we check if your #1 matches the human's #1</li>
    </ul>
    
    <p><strong>Key Point:</strong> The "best" image isn't arbitrary - it's the image that humans consistently preferred in head-to-head comparisons!</p>

    <h3>The Task</h3>
    <p><strong>Your algorithm must:</strong></p>
    <ol>
        <li>Look at ALL images in a series</li>
        <li>Assign a quality score to EACH image</li>
        <li>Rank them from best to worst</li>
        <li>We check: Did you rank the human-labeled "best" image at position #1?</li>
    </ol>
    <p><span class="highlight">This is NOT pairwise comparison</span> - you rank ALL images in the series simultaneously.</p>

    <h2>Concrete Example: Series 000002</h2>
    <p>Let's walk through a complete example with 8 images.</p>

    <h3>Step 1: The Series</h3>
    <pre>Series ID: 000002
Number of images: 8
Human-labeled best: 000002-03.JPG (determined from pairwise comparisons)</pre>
    
    <p><strong>Note:</strong> Based on the actual comparison data, 2-3.JPG (000002-03.JPG) appears to be the winner from the pairwise comparisons. It consistently beats other images in head-to-head comparisons.</p>

    <h3>Step 2: Your Algorithm Scores Each Image</h3>
    <p>Let's say your algorithm uses sharpness to score quality:</p>

    <table>
        <tr>
            <th>Image</th>
            <th>Sharpness Score</th>
            <th>Quality Score</th>
        </tr>
        <tr>
            <td>000002-01.JPG</td>
            <td>145.2</td>
            <td>0.725</td>
        </tr>
        <tr>
            <td>000002-02.JPG</td>
            <td>98.3</td>
            <td>0.492</td>
        </tr>
        <tr>
            <td>000002-03.JPG</td>
            <td>187.6</td>
            <td>0.938 (Highest - your pick AND human pick!)</td>
        </tr>
        <tr>
            <td>000002-04.JPG</td>
            <td>156.1</td>
            <td>0.781</td>
        </tr>
        <tr>
            <td>000002-05.JPG</td>
            <td>112.4</td>
            <td>0.562</td>
        </tr>
        <tr>
            <td>000002-06.JPG</td>
            <td>134.8</td>
            <td>0.674</td>
        </tr>
        <tr>
            <td>000002-07.JPG</td>
            <td>89.2</td>
            <td>0.446</td>
        </tr>
        <tr>
            <td>000002-08.JPG</td>
            <td>101.5</td>
            <td>0.508</td>
        </tr>
    </table>

    <h3>Step 3: Rank Images by Score</h3>
    <p>Your algorithm produces this ranking:</p>

    <table>
        <tr>
            <th>Rank</th>
            <th>Image</th>
            <th>Score</th>
            <th>Is Human Pick?</th>
        </tr>
        <tr>
            <td>1</td>
            <td>000002-03.JPG</td>
            <td>0.938</td>
            <td>YES (Human pick at rank 1 - perfect match!)</td>
        </tr>
        <tr>
            <td>2</td>
            <td>000002-04.JPG</td>
            <td>0.781</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>3</td>
            <td>000002-01.JPG</td>
            <td>0.725</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>4</td>
            <td>000002-06.JPG</td>
            <td>0.674</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>5</td>
            <td>000002-05.JPG</td>
            <td>0.562</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>6</td>
            <td>000002-08.JPG</td>
            <td>0.508</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>7</td>
            <td>000002-02.JPG</td>
            <td>0.492</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>8</td>
            <td>000002-07.JPG</td>
            <td>0.446</td>
            <td>NO</td>
        </tr>
    </table>

    <h3>Step 4: Evaluate Performance</h3>
    <p>For this ONE series, we calculate:</p>
    <ul>
        <li><strong>Top-1 Accuracy:</strong> Is best image (000002-03.JPG) at rank 1? YES! - Score: 1</li>
        <li><strong>Top-2 Accuracy:</strong> Is best image in top 2? YES (it's at rank 1) - Score: 1</li>
        <li><strong>Top-3 Accuracy:</strong> Is best image in top 3? YES (it's at rank 1) - Score: 1</li>
        <li><strong>Mean Reciprocal Rank (MRR):</strong> 1 / rank_of_best_image = 1 / 1 = 1.0 (perfect!)</li>
        <li><strong>Mean Rank:</strong> Just the rank = 1</li>
    </ul>
    
    <p><strong>Result:</strong> Your algorithm correctly identified the human-preferred image! This is a "hit" for Top-1 accuracy.</p>

    <h2>Understanding the Metrics</h2>

    <h3>Top-1 Accuracy (Most Important)</h3>
    <p><strong>What it measures:</strong> Percentage of series where your #1 pick matches the human's pick</p>
    <p><strong>Formula:</strong> (Number of series where best image is ranked #1) / (Total number of series)</p>

    <p><strong>Example over 5 series:</strong></p>
    <table>
        <tr>
            <th>Series</th>
            <th>Human Best</th>
            <th>Your Rank</th>
            <th>Top-1 Hit?</th>
        </tr>
        <tr>
            <td>000001</td>
            <td>01.JPG</td>
            <td>Rank 1</td>
            <td>YES</td>
        </tr>
        <tr>
            <td>000002</td>
            <td>04.JPG</td>
            <td>Rank 2</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>000003</td>
            <td>02.JPG</td>
            <td>Rank 1</td>
            <td>YES</td>
        </tr>
        <tr>
            <td>000004</td>
            <td>03.JPG</td>
            <td>Rank 3</td>
            <td>NO</td>
        </tr>
        <tr>
            <td>000005</td>
            <td>01.JPG</td>
            <td>Rank 1</td>
            <td>YES</td>
        </tr>
    </table>
    <p><strong>Top-1 Accuracy: 3/5 = 0.60 = 60%</strong></p>

    <p><strong>Interpretation:</strong></p>
    <ul>
        <li>60% = Your algorithm correctly picks the best photo 60% of the time</li>
        <li>100% = Perfect (always pick human's choice)</li>
        <li>Random guessing with 3 images = 33.3%</li>
    </ul>

    <h3>Top-2 Accuracy</h3>
    <p><strong>What it measures:</strong> Percentage of series where the human's pick is in your top 2</p>
    <p><strong>Why it matters:</strong> Shows how "close" you are when you don't get #1 exactly right</p>

    <h3>Mean Reciprocal Rank (MRR)</h3>
    <p><strong>What it measures:</strong> Average of 1/rank across all series</p>
    <p><strong>Formula for one series:</strong> 1 / rank_of_best_image</p>

    <p><strong>Example over 5 series:</strong></p>
    <table>
        <tr>
            <th>Series</th>
            <th>Human Best</th>
            <th>Your Rank</th>
            <th>Reciprocal Rank</th>
        </tr>
        <tr>
            <td>000001</td>
            <td>01.JPG</td>
            <td>Rank 1</td>
            <td>1/1 = 1.000</td>
        </tr>
        <tr>
            <td>000002</td>
            <td>04.JPG</td>
            <td>Rank 2</td>
            <td>1/2 = 0.500</td>
        </tr>
        <tr>
            <td>000003</td>
            <td>02.JPG</td>
            <td>Rank 1</td>
            <td>1/1 = 1.000</td>
        </tr>
        <tr>
            <td>000004</td>
            <td>03.JPG</td>
            <td>Rank 3</td>
            <td>1/3 = 0.333</td>
        </tr>
        <tr>
            <td>000005</td>
            <td>01.JPG</td>
            <td>Rank 1</td>
            <td>1/1 = 1.000</td>
        </tr>
    </table>
    <p><strong>MRR: (1.000 + 0.500 + 1.000 + 0.333 + 1.000) / 5 = 0.767</strong></p>

    <p><strong>Interpretation:</strong></p>
    <ul>
        <li>1.0 = Perfect (always rank best image #1)</li>
        <li>0.5 = On average, best image is ranked #2</li>
        <li>0.333 = On average, best image is ranked #3</li>
    </ul>

    <h2>How Dataset Size Affects Metrics</h2>

    <p>PhotoTriage has variable series sizes:</p>
    <ul>
        <li>Some series: 2 images</li>
        <li>Some series: 4 images</li>
        <li>Some series: 8 images (maximum)</li>
        <li>Average: 2.6 images</li>
    </ul>

    <p><strong>Impact on Difficulty:</strong></p>
    <ul>
        <li><strong>Series with 2 images:</strong> Random guessing = 50% chance (easier)</li>
        <li><strong>Series with 8 images:</strong> Random guessing = 12.5% chance (much harder)</li>
    </ul>

    <p><strong>Why This is Fair:</strong> All series are weighted equally regardless of size. Each series contributes 1 to the total count.</p>

    <h2>Our Benchmark Results Explained</h2>

    <h3>Sharpness-Only Method (Winner: 64.95%)</h3>
    <p><strong>What it does:</strong> Scores each image by sharpness (Laplacian variance), ranks by highest score</p>
    <p><strong>Results:</strong></p>
    <ul>
        <li>Top-1 Accuracy: 64.95%</li>
        <li>Top-2 Accuracy: 77.57%</li>
        <li>MRR: 0.789</li>
    </ul>
    <p><strong>Interpretation:</strong> In 64.95 out of 100 series, the sharpest photo was the one humans picked. On average, the human's pick was at rank 1.27 (very close to #1).</p>

    <h3>Contrast-Only Method (48.37%)</h3>
    <p><strong>Results:</strong></p>
    <ul>
        <li>Top-1 Accuracy: 48.37%</li>
        <li>Top-2 Accuracy: 83.23%</li>
    </ul>
    <p><strong>Interpretation:</strong> Only 48% of the time the highest contrast image was the best, BUT 83% of the time the best image had top-2 contrast. This suggests contrast helps, but it's not the PRIMARY factor.</p>

    <h3>Composite Method (42.05%)</h3>
    <p><strong>What it does:</strong> Combines sharpness (35%), exposure (25%), colorfulness (20%), contrast (15%)</p>
    <p><strong>Surprising Finding:</strong> Combining multiple factors made it WORSE than just using sharpness alone!</p>
    <p><strong>Why?</strong> Sharpness is what people care about most. Adding other factors "diluted" the sharpness signal.</p>

    <h2>Complete Example: Evaluating a Method</h2>

    <h3>The Dataset</h3>
    <pre>Series 000001 (4 images):
- 000001-01.JPG
- 000001-02.JPG (BEST - human labeled)
- 000001-03.JPG
- 000001-04.JPG

Series 000002 (8 images):
- 000002-01.JPG through 000002-08.JPG
- 000002-04.JPG (BEST - human labeled)

Series 000010 (2 images):
- 000010-01.JPG (BEST - human labeled)
- 000010-02.JPG</pre>

    <h3>Your Algorithm Results</h3>

    <p><strong>Series 000001:</strong> Human pick at rank 2</p>
    <p><strong>Series 000002:</strong> Human pick at rank 1 (correct!)</p>
    <p><strong>Series 000010:</strong> Human pick at rank 1 (correct!)</p>

    <h3>Final Metrics</h3>
    <ul>
        <li><strong>Top-1 Accuracy:</strong> 2/3 = 66.67%</li>
        <li><strong>Top-2 Accuracy:</strong> 3/3 = 100%</li>
        <li><strong>MRR:</strong> (0.5 + 1.0 + 1.0) / 3 = 0.833</li>
        <li><strong>Mean Rank:</strong> (2 + 1 + 1) / 3 = 1.333</li>
    </ul>

    <h2>Understanding the Ground Truth Creation Process</h2>
    
    <h3>How PhotoTriage Determines the "Best" Image</h3>
    <p>The PhotoTriage dataset uses a sophisticated method to determine which image is "best":</p>
    
    <h4>1. Pairwise Comparison Collection</h4>
    <p>For each series, humans were shown pairs of images and asked to choose the better one. This creates a dataset of comparisons like:</p>
    <pre>{"compareID1": 3, "compareID2": 2, 
 "compareFile1": "2-4.JPG", "compareFile2": "2-3.JPG",
 "userChoice": "RIGHT", 
 "reason": ["", "you can not see the water"]}</pre>
    
    <p>This means: When comparing 2-4.JPG (LEFT) vs 2-3.JPG (RIGHT), the human chose RIGHT (2-3.JPG) because 2-4.JPG doesn't show the water.</p>
    
    <h4>2. Ranking Algorithm</h4>
    <p>From all pairwise comparisons, a ranking algorithm (like Bradley-Terry model) calculates:</p>
    <ul>
        <li>Win count for each image</li>
        <li>Win rate (wins / total comparisons)</li>
        <li>Overall ranking score</li>
    </ul>
    
    <h4>3. Best Image Selection</h4>
    <p>The image with the highest ranking score becomes the "best" image for that series.</p>
    
    <h4>Example: Analyzing Series 000002 from JSON Data</h4>
    <p>Looking at the actual comparison JSON data, here's how to determine the best image:</p>
    
    <p><strong>Step 1: Parse Each Comparison</strong></p>
    <p>Each comparison has:</p>
    <ul>
        <li><code>compareFile1</code> and <code>compareFile2</code>: The two images being compared</li>
        <li><code>userChoice</code>: "LEFT" (prefers File1) or "RIGHT" (prefers File2)</li>
        <li><code>reason</code>: Why the human made that choice</li>
    </ul>
    
    <p><strong>Example comparison:</strong></p>
    <pre>{"compareID1": 3, "compareID2": 2,
 "compareFile1": "2-4.JPG", "compareFile2": "2-3.JPG",
 "userChoice": "RIGHT",
 "reason": ["", "you can not see the water"]}</pre>
    
    <p>This means: 2-3.JPG (RIGHT) beats 2-4.JPG (LEFT) because 2-4.JPG doesn't show water.</p>
    
    <p><strong>Step 2: Count Wins for Each Image</strong></p>
    <p>Go through all comparisons and count wins:</p>
    <ul>
        <li>If <code>userChoice == "LEFT"</code>: compareFile1 wins, compareFile2 loses</li>
        <li>If <code>userChoice == "RIGHT"</code>: compareFile2 wins, compareFile1 loses</li>
    </ul>
    
    <p><strong>Step 3: Calculate Win Rates</strong></p>
    <p>For each image: Win Rate = Wins / (Wins + Losses)</p>
    
    <p><strong>Step 4: Best Image = Highest Win Rate</strong></p>
    <p>The image with the highest win rate (or highest ranking score from Bradley-Terry model) is the "best" image.</p>
    
    <p><strong>Python Code to Analyze JSON:</strong></p>
    <pre>import json
from collections import defaultdict

# Load comparison data
with open('comparisons.json') as f:
    data = json.load(f)

# Count wins and losses for each image
wins = defaultdict(int)
losses = defaultdict(int)

for review in data['reviews']:
    file1 = review['compareFile1']
    file2 = review['compareFile2']
    choice = review['userChoice']
    
    if choice == 'LEFT':
        wins[file1] += 1
        losses[file2] += 1
    else:  # RIGHT
        wins[file2] += 1
        losses[file1] += 1

# Calculate win rates
win_rates = {}
for image in set(list(wins.keys()) + list(losses.keys())):
    total = wins[image] + losses[image]
    if total > 0:
        win_rates[image] = wins[image] / total

# Find best image
best_image = max(win_rates.items(), key=lambda x: x[1])
print(f"Best image: {best_image[0]} with {best_image[1]*100:.1f}% win rate")</pre>
    
    <p><strong>Common Patterns in the Data:</strong></p>
    <ul>
        <li>Images that "show water" often win over images that don't</li>
        <li>Images that are "too far away" or "too close" often lose</li>
        <li>Images with "bad composition" or "blurry" often lose</li>
        <li>Images with "better detail" or "better framing" often win</li>
    </ul>
    
    <p><strong>Key Insight:</strong> The "best" image isn't chosen arbitrarily - it's the image that humans consistently preferred when compared head-to-head with other images in the series. The pairwise comparison method is more reliable than asking humans to rank 8 images at once.</p>
    
    <h3>What This Means for Your Algorithm</h3>
    <p>Your algorithm doesn't see the pairwise comparisons - it only sees:</p>
    <ul>
        <li>All images in the series</li>
        <li>The final "best" image label (which image won the most comparisons)</li>
    </ul>
    <p>Your task: Rank all images and see if your #1 matches the human's #1 (the image that won the most pairwise comparisons).</p>
    
    <h2>Key Takeaways</h2>
    <ol>
        <li><strong>Ground Truth from Pairwise Comparisons:</strong> The "best" image is determined by aggregating many human pairwise preferences</li>
        <li><strong>Not Pairwise for You:</strong> Your algorithm ranks ALL images in each series simultaneously</li>
        <li><strong>Variable Sizes:</strong> Series have 1-8 images (average 2.6)</li>
        <li><strong>One Ground Truth:</strong> Each series has exactly ONE human-labeled best image (the pairwise winner)</li>
        <li><strong>Top-1 is Key:</strong> The most important metric - did you pick the same image that won the most comparisons?</li>
        <li><strong>Sharpness Wins:</strong> On PhotoTriage, sharpness alone beats complex composite methods</li>
        <li><strong>People Prefer Sharp Photos:</strong> This is the main insight from the dataset</li>
    </ol>

    <h2>How to Use This Dataset</h2>

    <h3>For Development</h3>
    <pre>python run_quality_benchmark.py configs/quality_benchmark.quick.yaml</pre>

    <h3>For Final Evaluation</h3>
    <pre>python run_quality_benchmark.py configs/quality_benchmark.phototriage.yaml</pre>

    <h3>For Custom Methods</h3>
    <pre>from sim_bench.quality_assessment import QualityEvaluator
from sim_bench.datasets import load_dataset

dataset = load_dataset('phototriage', config)
dataset.load_data()

class MyQualityMethod:
    def assess_image(self, image_path):
        return score

evaluator = QualityEvaluator(dataset, MyQualityMethod())
results = evaluator.evaluate()
print(f"Top-1 Accuracy: {results['metrics']['top1_accuracy']}")</pre>

    <h2>Related Documentation</h2>
    <ul>
        <li>Quality Assessment Quickstart: quickstart.md</li>
        <li>Benchmark Guide: benchmark.md</li>
        <li>PhotoTriage Dataset: ../image_similarity/datasets_phototriage.md</li>
    </ul>

</body>
</html>

