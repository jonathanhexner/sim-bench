{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 Multitask Training Analysis\n",
    "\n",
    "Analysis of training run: `outputs/phase2_multitask/20260125_020510`\n",
    "\n",
    "## Contents\n",
    "1. Training Metrics (Loss, Accuracy, Error) vs Epoch\n",
    "2. Dataset Distribution Analysis\n",
    "3. Understanding the Negative Loss Issue\n",
    "4. Suggestions for Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = Path(r'D:\\sim-bench\\outputs\\phase2_multitask\\20260125_020510')\n",
    "DATASET_DIR = Path(r'D:/DataSets/AffectNet/train')\n",
    "LANDMARKS_CACHE = Path(r'D:\\sim-bench\\cache\\affectnet_landmarks.json')\n",
    "\n",
    "# Expression class mapping\n",
    "EXPRESSION_NAMES = {\n",
    "    0: 'neutral',\n",
    "    1: 'happy',\n",
    "    2: 'sad',\n",
    "    3: 'surprise',\n",
    "    4: 'fear',\n",
    "    5: 'disgust',\n",
    "    6: 'anger',\n",
    "    7: 'contempt'\n",
    "}\n",
    "\n",
    "# Folder to class mapping (AffectNet folder names)\n",
    "FOLDER_TO_CLASS = {\n",
    "    'neutral': 0,\n",
    "    'happy': 1, 'happiness': 1,\n",
    "    'sad': 2, 'sadness': 2,\n",
    "    'surprise': 3, 'surprised': 3,\n",
    "    'fear': 4, 'fearful': 4,\n",
    "    'disgust': 5, 'disgusted': 5,\n",
    "    'anger': 6, 'angry': 6,\n",
    "    'contempt': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "metrics_df = pd.read_csv(OUTPUT_DIR / 'metrics.csv')\n",
    "print(f\"Loaded {len(metrics_df)} epochs of metrics\")\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs completed: {metrics_df['epoch'].max()}\")\n",
    "print(f\"\\nExpression Accuracy:\")\n",
    "print(f\"  Train: {metrics_df['train_expr_acc'].iloc[0]:.2f}% -> {metrics_df['train_expr_acc'].iloc[-1]:.2f}%\")\n",
    "print(f\"  Val:   {metrics_df['val_expr_acc'].iloc[0]:.2f}% -> {metrics_df['val_expr_acc'].iloc[-1]:.2f}%\")\n",
    "print(f\"  Best Val: {metrics_df['val_expr_acc'].max():.2f}% (epoch {metrics_df['val_expr_acc'].idxmax() + 1})\")\n",
    "print(f\"\\nLandmark Error:\")\n",
    "print(f\"  Train: {metrics_df['train_lm_error'].iloc[0]:.4f} -> {metrics_df['train_lm_error'].iloc[-1]:.4f}\")\n",
    "print(f\"  Val:   {metrics_df['val_lm_error'].iloc[0]:.4f} -> {metrics_df['val_lm_error'].iloc[-1]:.4f}\")\n",
    "print(f\"\\nLoss (note: negative due to uncertainty weighting):\")\n",
    "print(f\"  Train: {metrics_df['train_loss'].iloc[0]:.4f} -> {metrics_df['train_loss'].iloc[-1]:.4f}\")\n",
    "print(f\"  Val:   {metrics_df['val_loss'].iloc[0]:.4f} -> {metrics_df['val_loss'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Combined Loss\n",
    "ax = axes[0]\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_loss'], 'b-o', label='Train', markersize=4)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_loss'], 'r-s', label='Val', markersize=4)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Combined Loss (Uncertainty Weighted)\\n⚠️ Negative loss is expected - see explanation below')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed in - excluding epoch 1 to see convergence behavior\n",
    "ax = axes[1]\n",
    "ax.plot(metrics_df['epoch'][1:], metrics_df['train_loss'][1:], 'b-o', label='Train', markersize=4)\n",
    "ax.plot(metrics_df['epoch'][1:], metrics_df['val_loss'][1:], 'r-s', label='Val', markersize=4)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss (Epochs 2+, Zoomed)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'loss_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Expression Accuracy vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_expr_acc'], 'b-o', label='Train', markersize=5)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_expr_acc'], 'r-s', label='Validation', markersize=5)\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=12.5, color='gray', linestyle='--', alpha=0.7, label='Random (1/8 = 12.5%)')\n",
    "ax.axhline(y=100/8, color='orange', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Expression Accuracy (%)')\n",
    "ax.set_title('Expression Classification Accuracy\\n(8 classes: random = 12.5%)')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 50])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'expression_accuracy_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n⚠️ ISSUE: Accuracy plateaus at ~28% - only ~2x random chance\")\n",
    "print(f\"   This suggests the model is not learning meaningful expression features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Landmark Error vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_lm_error'], 'b-o', label='Train', markersize=5)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_lm_error'], 'r-s', label='Validation', markersize=5)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Landmark MSE Error')\n",
    "ax.set_title('Landmark Regression Error (MSE)\\n5-point landmarks, normalized [0,1] coordinates')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'landmark_error_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLandmark error is very low (~0.014 MSE = ~0.12 normalized error)\")\n",
    "print(f\"The model learns landmarks very easily - this task may be too simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Uncertainty Weights Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'expression_weight' in metrics_df.columns and 'landmark_weight' in metrics_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Weights over time\n",
    "    ax = axes[0]\n",
    "    ax.plot(metrics_df['epoch'], metrics_df['expression_weight'], 'g-o', label='Expression Weight', markersize=4)\n",
    "    ax.plot(metrics_df['epoch'], metrics_df['landmark_weight'], 'm-s', label='Landmark Weight', markersize=4)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Weight (precision = 1/σ²)')\n",
    "    ax.set_title('Learned Uncertainty Weights\\n(Higher = model is more confident)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Weight ratio\n",
    "    ax = axes[1]\n",
    "    weight_ratio = metrics_df['landmark_weight'] / metrics_df['expression_weight']\n",
    "    ax.plot(metrics_df['epoch'], weight_ratio, 'k-o', markersize=4)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Landmark/Expression Weight Ratio')\n",
    "    ax.set_title('Task Weight Imbalance\\n(>1 means landmark task dominates)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'uncertainty_weights_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n⚠️ ISSUE: Landmark weight explodes to ~{metrics_df['landmark_weight'].iloc[-1]:.0f}x\")\n",
    "    print(f\"   While expression weight stays at ~{metrics_df['expression_weight'].iloc[-1]:.2f}\")\n",
    "    print(f\"   This means the model finds landmarks trivially easy, expression very hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(metrics_df['epoch'], metrics_df['learning_rate'], 'b-o', markersize=4)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule (Cosine Annealing with Warmup)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per expression class\n",
    "class_counts = {}\n",
    "\n",
    "for folder in DATASET_DIR.iterdir():\n",
    "    if folder.is_dir():\n",
    "        folder_name = folder.name.lower()\n",
    "        if folder_name in FOLDER_TO_CLASS:\n",
    "            class_id = FOLDER_TO_CLASS[folder_name]\n",
    "            class_name = EXPRESSION_NAMES[class_id]\n",
    "            count = len(list(folder.glob('*.jpg'))) + len(list(folder.glob('*.png')))\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + count\n",
    "\n",
    "# Create DataFrame\n",
    "dist_df = pd.DataFrame([\n",
    "    {'class_id': i, 'class_name': EXPRESSION_NAMES[i], 'count': class_counts.get(EXPRESSION_NAMES[i], 0)}\n",
    "    for i in range(8)\n",
    "])\n",
    "dist_df['percentage'] = dist_df['count'] / dist_df['count'].sum() * 100\n",
    "\n",
    "print(\"Dataset Class Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "print(dist_df.to_string(index=False))\n",
    "print(f\"\\nTotal images: {dist_df['count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "ax = axes[0]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, 8))\n",
    "bars = ax.bar(dist_df['class_name'], dist_df['count'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xlabel('Expression Class')\n",
    "ax.set_ylabel('Number of Images')\n",
    "ax.set_title('Dataset Distribution by Expression')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, dist_df['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "            f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Add balanced reference line\n",
    "balanced = dist_df['count'].sum() / 8\n",
    "ax.axhline(y=balanced, color='red', linestyle='--', alpha=0.7, label=f'Balanced ({balanced:.0f})')\n",
    "ax.legend()\n",
    "\n",
    "# Pie chart\n",
    "ax = axes[1]\n",
    "ax.pie(dist_df['count'], labels=dist_df['class_name'], autopct='%1.1f%%', \n",
    "       colors=colors, startangle=90)\n",
    "ax.set_title('Expression Distribution (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Imbalance analysis\n",
    "max_count = dist_df['count'].max()\n",
    "min_count = dist_df['count'].min()\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nImbalance Analysis:\")\n",
    "print(f\"  Max class: {dist_df.loc[dist_df['count'].idxmax(), 'class_name']} ({max_count:,} images)\")\n",
    "print(f\"  Min class: {dist_df.loc[dist_df['count'].idxmin(), 'class_name']} ({min_count:,} images)\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Landmarks Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load landmarks cache (may take a moment)\n",
    "print(\"Loading landmarks cache...\")\n",
    "with open(LANDMARKS_CACHE, 'r') as f:\n",
    "    landmarks_data = json.load(f)\n",
    "\n",
    "print(f\"Total landmarks entries: {len(landmarks_data):,}\")\n",
    "\n",
    "# Count landmarks per expression class\n",
    "landmarks_per_class = {name: {'with_landmarks': 0, 'without_landmarks': 0} for name in EXPRESSION_NAMES.values()}\n",
    "\n",
    "for folder in DATASET_DIR.iterdir():\n",
    "    if folder.is_dir():\n",
    "        folder_name = folder.name.lower()\n",
    "        if folder_name in FOLDER_TO_CLASS:\n",
    "            class_name = EXPRESSION_NAMES[FOLDER_TO_CLASS[folder_name]]\n",
    "            for img_path in folder.glob('*.jpg'):\n",
    "                # Check both forward and backward slash versions\n",
    "                path_str = str(img_path)\n",
    "                alt_path_str = path_str.replace('\\\\', '/')\n",
    "                \n",
    "                if path_str in landmarks_data or alt_path_str in landmarks_data:\n",
    "                    landmarks_per_class[class_name]['with_landmarks'] += 1\n",
    "                else:\n",
    "                    landmarks_per_class[class_name]['without_landmarks'] += 1\n",
    "\n",
    "# Summary\n",
    "print(\"\\nLandmarks coverage per class:\")\n",
    "for class_name, counts in landmarks_per_class.items():\n",
    "    total = counts['with_landmarks'] + counts['without_landmarks']\n",
    "    coverage = counts['with_landmarks'] / total * 100 if total > 0 else 0\n",
    "    print(f\"  {class_name:10s}: {counts['with_landmarks']:5d}/{total:5d} ({coverage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Negative Loss\n",
    "\n",
    "### Why is `train_loss` negative?\n",
    "\n",
    "The model uses **Uncertainty Weighting** (Kendall et al., 2018) for multi-task learning:\n",
    "\n",
    "$$L_{total} = \\sum_i \\left[ \\frac{1}{2\\sigma_i^2} L_i + \\frac{1}{2} \\log \\sigma_i^2 \\right]$$\n",
    "\n",
    "Where:\n",
    "- $L_i$ = task loss (expression or landmark)\n",
    "- $\\sigma_i^2$ = learned uncertainty/variance for task $i$\n",
    "- $\\log \\sigma_i^2$ = log variance (learnable parameter)\n",
    "\n",
    "**The issue:**\n",
    "When $\\log \\sigma_i^2 < 0$ (meaning $\\sigma_i^2 < 1$, i.e., high precision/confidence), the regularization term $\\frac{1}{2} \\log \\sigma_i^2$ becomes **negative**.\n",
    "\n",
    "If the negative regularization term outweighs the positive weighted loss term, the **total loss becomes negative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the uncertainty weighting math\n",
    "if 'expression_log_var' in metrics_df.columns:\n",
    "    print(\"Uncertainty Weighting Breakdown (last epoch):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    expr_log_var = metrics_df['expression_log_var'].iloc[-1]\n",
    "    lm_log_var = metrics_df['landmark_log_var'].iloc[-1]\n",
    "    \n",
    "    print(f\"\\nExpression task:\")\n",
    "    print(f\"  log_var (log σ²) = {expr_log_var:.4f}\")\n",
    "    print(f\"  σ² = exp({expr_log_var:.4f}) = {np.exp(expr_log_var):.4f}\")\n",
    "    print(f\"  precision (1/σ²) = {np.exp(-expr_log_var):.4f}\")\n",
    "    print(f\"  regularization = 0.5 * log_var = {0.5 * expr_log_var:.4f}\")\n",
    "    \n",
    "    print(f\"\\nLandmark task:\")\n",
    "    print(f\"  log_var (log σ²) = {lm_log_var:.4f}\")\n",
    "    print(f\"  σ² = exp({lm_log_var:.4f}) = {np.exp(lm_log_var):.6f}\")\n",
    "    print(f\"  precision (1/σ²) = {np.exp(-lm_log_var):.2f}\")\n",
    "    print(f\"  regularization = 0.5 * log_var = {0.5 * lm_log_var:.4f}  <-- NEGATIVE!\")\n",
    "    \n",
    "    print(f\"\\nTotal regularization contribution: {0.5 * expr_log_var + 0.5 * lm_log_var:.4f}\")\n",
    "    print(f\"\\n⚠️ The landmark regularization term ({0.5 * lm_log_var:.4f}) is strongly negative\")\n",
    "    print(f\"   because the model is very confident about landmarks (σ² ≈ 0.01)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is negative loss a problem?\n",
    "\n",
    "**Short answer: Not inherently, but it's a symptom.**\n",
    "\n",
    "The negative loss itself doesn't break optimization (gradients still flow correctly). However, it indicates:\n",
    "\n",
    "1. **Task imbalance**: Landmark task is too easy compared to expression\n",
    "2. **Uncertainty collapse**: The model becomes overconfident on landmarks\n",
    "3. **Gradient imbalance**: Expression task may be under-trained\n",
    "\n",
    "**Recommendations:**\n",
    "- Consider decoupling the tasks or using fixed weights\n",
    "- Add regularization to prevent log_var from going too negative\n",
    "- Use a harder landmark task (more points, harder samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary & Suggestions\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Expression accuracy plateaus at ~28%** (barely 2x random)\n",
    "   - Suggests the model isn't learning meaningful expression features\n",
    "   - Could be data quality, architecture, or learning rate issues\n",
    "\n",
    "2. **Landmark task is too easy**\n",
    "   - Error quickly converges to ~0.014 MSE\n",
    "   - Uncertainty weight explodes to 70x\n",
    "   - Model focuses on landmarks at expense of expression\n",
    "\n",
    "3. **Training stopped at epoch 24/30**\n",
    "   - Likely memory issue on CPU training\n",
    "   - Consider using GPU or reducing batch size\n",
    "\n",
    "### Suggestions for Improvement:\n",
    "\n",
    "1. **Add per-image validation predictions** (your idea is critical!)\n",
    "   - Save predictions for every validation image each epoch\n",
    "   - Enables confusion matrix, error analysis by class\n",
    "   - Can identify systematic failures\n",
    "\n",
    "2. **Confusion matrix analysis**\n",
    "   - Which expressions are confused with each other?\n",
    "   - Is the model just predicting majority class?\n",
    "\n",
    "3. **Visualize failure cases**\n",
    "   - What do misclassified images look like?\n",
    "   - Are there annotation errors in the dataset?\n",
    "\n",
    "4. **Consider architectural changes:**\n",
    "   - Separate backbones for each task?\n",
    "   - Different loss weighting strategy?\n",
    "   - Focal loss for class imbalance?\n",
    "\n",
    "5. **Training diagnostics to add:**\n",
    "   - Per-class accuracy breakdown\n",
    "   - Gradient norms per layer\n",
    "   - Feature visualization (t-SNE of embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_loss'], 'b-o', label='Train', markersize=3)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_loss'], 'r-s', label='Val', markersize=3)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss (Uncertainty Weighted)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Expression Accuracy\n",
    "ax = axes[0, 1]\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_expr_acc'], 'b-o', label='Train', markersize=3)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_expr_acc'], 'r-s', label='Val', markersize=3)\n",
    "ax.axhline(y=12.5, color='gray', linestyle='--', alpha=0.7, label='Random')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Expression Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Landmark Error\n",
    "ax = axes[1, 0]\n",
    "ax.plot(metrics_df['epoch'], metrics_df['train_lm_error'], 'b-o', label='Train', markersize=3)\n",
    "ax.plot(metrics_df['epoch'], metrics_df['val_lm_error'], 'r-s', label='Val', markersize=3)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Error')\n",
    "ax.set_title('Landmark Error')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Dataset Distribution\n",
    "ax = axes[1, 1]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, 8))\n",
    "ax.bar(dist_df['class_name'], dist_df['count'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xlabel('Expression')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Dataset Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis complete! Plots saved to output directory.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
