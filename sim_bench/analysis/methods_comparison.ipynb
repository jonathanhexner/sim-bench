{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Overview: Method Comparison\n",
    "\n",
    "Compare multiple similarity methods side-by-side:\n",
    "- **Summary statistics** for each method\n",
    "- **Performance rankings** across metrics\n",
    "- **Method correlations** (do methods agree?)\n",
    "- **Metric distributions** and outlier analysis\n",
    "\n",
    "For detailed single-method analysis with query visualization, see `method_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\sim-bench\\.venv\\Scripts\\python.exe\n",
      "['d:\\\\sim-bench', 'd:\\\\sim-bench', 'd:\\\\sim-bench', 'C:\\\\Program Files\\\\Python311\\\\python311.zip', 'C:\\\\Program Files\\\\Python311\\\\DLLs', 'C:\\\\Program Files\\\\Python311\\\\Lib', 'C:\\\\Program Files\\\\Python311', 'd:\\\\sim-bench\\\\.venv', '', 'd:\\\\sim-bench\\\\.venv\\\\Lib\\\\site-packages', 'd:\\\\sim-bench\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'd:\\\\sim-bench\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'd:\\\\sim-bench\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared 8 cached modules\n",
      "sys.path[0] = d:\\sim-bench\n"
     ]
    }
   ],
   "source": [
    "# Force clean import\n",
    "import sys\n",
    "\n",
    "# Add project root\n",
    "sys.path.insert(0, 'd:\\\\sim-bench')\n",
    "\n",
    "# Nuclear option: Clear ALL sim_bench related modules\n",
    "to_delete = [key for key in sys.modules.keys() if key.startswith('sim_bench')]\n",
    "for key in to_delete:\n",
    "    del sys.modules[key]\n",
    "\n",
    "print(f\"Cleared {len(to_delete)} cached modules\")\n",
    "print(f\"sys.path[0] = {sys.path[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset\n",
    "from sim_bench.analysis.utils import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Method 'dinov2' not found in D:\\sim-bench\\outputs\\baseline_runs\\comprehensive_baseline\\unified_benchmark_2025-11-01_23-59-29\nAvailable: ['holidays_results_2025-11-02_00-07-35', 'ukbench_results_2025-11-01_23-59-29']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (EXPERIMENT_DIR / method).exists():\n\u001b[32m     32\u001b[39m         available = [d.name \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m EXPERIMENT_DIR.iterdir() \u001b[38;5;28;01mif\u001b[39;00m d.is_dir() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m d.name.startswith(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     34\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMethod \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPERIMENT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         )\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Configuration validated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPERIMENT_DIR.relative_to(PROJECT_ROOT)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Method 'dinov2' not found in D:\\sim-bench\\outputs\\baseline_runs\\comprehensive_baseline\\unified_benchmark_2025-11-01_23-59-29\nAvailable: ['holidays_results_2025-11-02_00-07-35', 'ukbench_results_2025-11-01_23-59-29']"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sim_bench.analysis.utils import get_project_root\n",
    "from sim_bench.analysis.io import load_metrics, load_per_query\n",
    "from sim_bench.analysis.export import export_notebook_to_pdf, archive_notebook\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# === Configuration ===\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "# Experiment settings\n",
    "\n",
    "# EXPERIMENT_DIR = PROJECT_ROOT / \"outputs\" / \"baseline_runs\" / \"comprehensive_baseline\" / \"2025-10-08_16-25-49\"\n",
    "EXPERIMENT_DIR = PROJECT_ROOT / \"outputs\" / \"baseline_runs\" / \"comprehensive_baseline\" / \"unified_benchmark_2025-11-01_23-59-29\"\n",
    "METHODS = [\"dinov2\"]  # Methods to compare\n",
    "\n",
    "# === Output Options ===\n",
    "EXPORT_PDF = True  # Export entire notebook to PDF\n",
    "\n",
    "# === Validation ===\n",
    "if not EXPERIMENT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "\n",
    "# Validate all methods exist\n",
    "for method in METHODS:\n",
    "    if not (EXPERIMENT_DIR / method).exists():\n",
    "        available = [d.name for d in EXPERIMENT_DIR.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "        raise FileNotFoundError(\n",
    "            f\"Method '{method}' not found in {EXPERIMENT_DIR}\\n\"\n",
    "            f\"Available: {available}\"\n",
    "        )\n",
    "\n",
    "print(f\"‚úì Configuration validated\")\n",
    "print(f\"  Experiment: {EXPERIMENT_DIR.relative_to(PROJECT_ROOT)}\")\n",
    "print(f\"  Methods: {', '.join(METHODS)}\")\n",
    "print(f\"  Export PDF: {EXPORT_PDF}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics for all methods\n",
    "metrics_data = {}\n",
    "for method in METHODS:\n",
    "    try:\n",
    "        df = load_metrics(method, EXPERIMENT_DIR)\n",
    "        metrics_data[method] = df\n",
    "        print(f\"‚úì {method}: {len(df)} metrics\")\n",
    "        # Handle backward compatibility\n",
    "        if 'map_full' in df.columns and 'map' not in df.columns:\n",
    "            df['map'] = df['map_full']\n",
    "        if 'prec@10' in df.columns and 'precision@10' not in df.columns:\n",
    "            df['precision@10'] = df['prec@10']        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö† {method}: metrics.csv not found, skipping\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_rows = []\n",
    "for method, df in metrics_data.items():\n",
    "    row = {'method': method}\n",
    "    row.update(df.iloc[0].to_dict())\n",
    "    summary_rows.append(row)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "print(f\"\\n‚úì Loaded {len(metrics_data)} methods\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics for comparison\n",
    "key_metrics = ['recall@1', 'recall@10', 'map@10']\n",
    "available_metrics = [m for m in key_metrics if m in df_summary.columns]\n",
    "\n",
    "if not available_metrics:\n",
    "    # Fallback: use all numeric columns except method\n",
    "    available_metrics = df_summary.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Create comparison table\n",
    "df_comparison = df_summary[['method'] + available_metrics].copy()\n",
    "df_comparison = df_comparison.set_index('method')\n",
    "\n",
    "# Highlight best values\n",
    "print(\"Performance Comparison (higher is better):\")\n",
    "df_comparison.style.highlight_max(axis=0, color='green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance across methods\n",
    "if len(available_metrics) > 0:\n",
    "    fig, axes = plt.subplots(1, min(4, len(available_metrics)), figsize=(4 * min(4, len(available_metrics)), 4))\n",
    "    if len(available_metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:4]):\n",
    "        ax = axes[idx]\n",
    "        df_comparison[metric].plot(kind='bar', ax=ax, color='steelblue')\n",
    "        ax.set_title(f'{metric.upper()}')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† No metrics available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(methods, experiment_dir, metric='ap@10', figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Compare a metric across methods using box plots.\n",
    "    \n",
    "    Args:\n",
    "        methods: List of method names\n",
    "        experiment_dir: Path to experiment directory\n",
    "        metric: Metric column name to compare\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Load per-query data for all methods\n",
    "    data = {}\n",
    "    for method in methods:\n",
    "        df = load_per_query(method, experiment_dir)\n",
    "        data[method] = df[metric].values\n",
    "    \n",
    "    # Create box plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    data_to_plot = [data[method] for method in methods]\n",
    "    bp = ax.boxplot(data_to_plot, labels=methods, patch_artist=True)\n",
    "    \n",
    "    # Color boxes\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(methods)]):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "    ax.set_xlabel('Method', fontsize=12)\n",
    "    ax.set_title(f'{metric.upper()} Distribution by Method', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{metric.upper()} Statistics:\")\n",
    "    for method in methods:\n",
    "        values = data[method]\n",
    "        print(f\"{method:15s} | Mean: {values.mean():.4f} | Median: {np.median(values):.4f} | Std: {values.std():.4f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Use it\n",
    "plot_metric_comparison(METHODS, EXPERIMENT_DIR, metric='ap@10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank methods by each metric\n",
    "df_ranks = df_comparison.rank(ascending=False, method='min').astype(int)\n",
    "df_ranks.columns = [f'{col}_rank' for col in df_ranks.columns]\n",
    "\n",
    "# Add average rank\n",
    "df_ranks['avg_rank'] = df_ranks.mean(axis=1)\n",
    "df_ranks = df_ranks.sort_values('avg_rank')\n",
    "\n",
    "print(\"Method Rankings (1=best):\")\n",
    "df_ranks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metric = 'ap@10'\n",
    "# Load per-query results to analyze correlations\n",
    "per_query_data = {}\n",
    "for method in METHODS:\n",
    "    try:\n",
    "        df = load_per_query(method, EXPERIMENT_DIR)\n",
    "        per_query_data[method] = df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö† {method}: per_query.csv not found\")\n",
    "\n",
    "if len(per_query_data) >= 2:\n",
    "    # Choose a metric to compare (e.g., first available numeric column)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    df_compare = pd.DataFrame()\n",
    "    for method, df in per_query_data.items():\n",
    "        if compare_metric in df.columns:\n",
    "            df_compare[method] = df[compare_metric].values\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr_matrix = df_compare.corr()\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "                vmin=-1, vmax=1, square=True)\n",
    "    plt.title(f'Method Correlation ({compare_metric})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCorrelation analysis based on: {compare_metric}\")\n",
    "    print(\"High correlation = methods agree on query difficulty\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö† Need at least 2 methods for correlation analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment: {EXPERIMENT_DIR.name}\")\n",
    "print(f\"Methods compared: {len(METHODS)}\")\n",
    "print(f\"\\nBest method by average rank: {df_ranks.index[0]}\")\n",
    "print(f\"\\nFor detailed analysis of a specific method, see method_analysis.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to PDF (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export to PDF and Archive Notebook (Optional) ===\n",
    "if EXPORT_PDF:\n",
    "    notebook_path = PROJECT_ROOT / \"sim_bench\" / \"analysis\" / \"methods_comparison.ipynb\"\n",
    "    output_dir = EXPERIMENT_DIR / \"analysis_reports\"\n",
    "    \n",
    "    # Archive the notebook (preserves configuration and code)\n",
    "    try:\n",
    "        archived_path = archive_notebook(notebook_path, output_dir=output_dir, prefix=\"methods_comparison\")\n",
    "        print(f\"‚úì Archived notebook: {archived_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Notebook archive failed: {e}\")\n",
    "    \n",
    "    # Export to PDF\n",
    "    try:\n",
    "        pdf_path = export_notebook_to_pdf(notebook_path, output_dir=output_dir, prefix=\"methods_comparison\")\n",
    "        print(f\"‚úì Exported to PDF: {pdf_path.name}\")\n",
    "        print(f\"\\nüìÅ Output location: {output_dir.relative_to(PROJECT_ROOT)}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† PDF export requires: pip install nbconvert[webpdf]\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† PDF export failed: {e}\")\n",
    "else:\n",
    "    print(\"PDF export disabled. Set EXPORT_PDF = True in configuration to enable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
